{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that translates the {input_language} to {output_language}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to german\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=TEMPLATE\n",
    ")\n",
    "prompt_template.format(input_language=\"english\", output_language=\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to german\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template=TEMPLATE, input_variables=[\"input_language\", \"output_language\"])\n",
    "prompt_template.format(input_language=\"english\", output_language=\"german\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Shot Prompt - provide a few examples in the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\n",
    "Examples:\n",
    "text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
    "sentiment: positive\n",
    "subject: BellaVista\n",
    "\n",
    "text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
    "sentiment: neutral\n",
    "subject: BellaVista\n",
    "\n",
    "text: I was disappointed with BellaVista. The service was slow and the dishes lacked flavor.\n",
    "sentiment: negative\n",
    "subject: BellaVista\n",
    "\n",
    "text: SeoulSavor offered the most authentic Korean flavors I've tasted outside of Seoul. The kimchi was perfectly fermented and spicy.\n",
    "sentiment: positive\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\n",
    "sentiment: neutral\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: I didn't enjoy my meal at SeoulSavor. The tteokbokki was too mushy and the service was not attentive.\n",
    "sentiment: negative\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: MunichMeals has the best bratwurst and sauerkraut I've tasted outside of Bavaria. Their beer garden ambiance is truly authentic.\n",
    "sentiment: positive\n",
    "subject: MunichMeals\n",
    "\n",
    "text: MunichMeals was alright. The weisswurst was okay, but I've had better elsewhere.\n",
    "sentiment: neutral\n",
    "subject: MunichMeals\n",
    "\n",
    "text: I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\n",
    "sentiment: negative\n",
    "subject: MunichMeals\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nInterprete the text and evaluate the text.\\nsentiment: is the text in a positive, neutral or negative sentiment?\\nsubject: What subject is the text about? Use exactly one word.\\n\\nFormat the output as JSON with the following keys:\\nsentiment\\nsubject\\n\\ntext: The MunichDeals experience was just awesome!\\n\\nExamples:\\ntext: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\\nsentiment: positive\\nsubject: BellaVista\\n\\ntext: BellaVista restaurant was alright. The food was decent, but nothing stood out.\\nsentiment: neutral\\nsubject: BellaVista\\n\\ntext: I was disappointed with BellaVista. The service was slow and the dishes lacked flavor.\\nsentiment: negative\\nsubject: BellaVista\\n\\ntext: SeoulSavor offered the most authentic Korean flavors I've tasted outside of Seoul. The kimchi was perfectly fermented and spicy.\\nsentiment: positive\\nsubject: SeoulSavor\\n\\ntext: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\\nsentiment: neutral\\nsubject: SeoulSavor\\n\\ntext: I didn't enjoy my meal at SeoulSavor. The tteokbokki was too mushy and the service was not attentive.\\nsentiment: negative\\nsubject: SeoulSavor\\n\\ntext: MunichMeals has the best bratwurst and sauerkraut I've tasted outside of Bavaria. Their beer garden ambiance is truly authentic.\\nsentiment: positive\\nsubject: MunichMeals\\n\\ntext: MunichMeals was alright. The weisswurst was okay, but I've had better elsewhere.\\nsentiment: neutral\\nsubject: MunichMeals\\n\\ntext: I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\\nsentiment: negative\\nsubject: MunichMeals\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template=TEMPLATE, input_variables=[\"input\"])\n",
    "prompt_template.format(input=\"The MunichDeals experience was just awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain also provides a FewShotPromptTemplate class, which allows creating the examples more modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\",\n",
    "        \"response\": \"sentiment: positive\\nsubject: BellaVista\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"BellaVista restaurant was alright. The food was decent, but nothing stood out.\",\n",
    "        \"response\": \"sentiment: neutral\\nsubject: BellaVista\"\n",
    "    },\n",
    "    ### other examples are left out\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = {\n",
    "    \"text\": \"SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\",\n",
    "    \"response\": \"sentiment: neutral\\nsubject: SeoulSavor\"\n",
    "}\n",
    "examples.append(new_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"text\", \"response\"], template=\"Text: {text}\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"text: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
      "sentiment: positive\n",
      "subject: BellaVista\n",
      "\n",
      "Text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
      "sentiment: neutral\n",
      "subject: BellaVista\n",
      "\n",
      "Text: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\n",
      "sentiment: neutral\n",
      "subject: SeoulSavor\n",
      "\n",
      "text: The MunichDeals experience was just awesome!\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input=\"The MunichDeals experience was just awesome!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compose multiple prompts together. This can be especially useful if you want to reuse parts of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Introduction\n",
    "introduction_template = \"\"\"\n",
    "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. Also, identify the subject of the text in one word.\n",
    "\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "# Example\n",
    "example_template = \"\"\"\n",
    "Chain-of-Thought Prompts:\n",
    "Let's start by evaluating a statement. Consider: \"{example_text}\". How does this make you feel about {example_subject}?\n",
    "Response: {example_evaluation}\n",
    "\n",
    "Based on the {example_sentiment} nature of that statement, how would you format your response?\n",
    "Response: {example_format}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# Execution\n",
    "execution_template = \"\"\"\n",
    "Now, execute this process for the text: \"{input}\".\n",
    "\"\"\"\n",
    "execution_prompt = PromptTemplate.from_template(execution_template)\n",
    "\n",
    "# Composing the full prompt\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{execution}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "# PipelinePrompts\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"execution\", execution_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. Also, identify the subject of the text in one word.\n",
      "\n",
      "\n",
      "\n",
      "Chain-of-Thought Prompts:\n",
      "Let's start by evaluating a statement. Consider: \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\". How does this make you feel about BellaVista?\n",
      "Response: It sounds like a positive review for BellaVista.\n",
      "\n",
      "Based on the positive nature of that statement, how would you format your response?\n",
      "Response: { \"sentiment\": \"positive\", \"subject\": \"BellaVista\" }\n",
      "\n",
      "\n",
      "\n",
      "Now, execute this process for the text: \"The new restaurant downtown has bland dishes and the wait time is too long.\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_prompt.format(\n",
    "    example_text=\"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\",\n",
    "    example_subject=\"BellaVista\",\n",
    "    example_evaluation=\"It sounds like a positive review for BellaVista.\",\n",
    "    example_sentiment=\"positive\",\n",
    "    example_format='{ \"sentiment\": \"positive\", \"subject\": \"BellaVista\" }',\n",
    "    input=\"The new restaurant downtown has bland dishes and the wait time is too long.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing prompts\n",
    "Example is an easy prompt since serializing does not work for the PipelinePromptTemplate (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"input\"], template=\"Tell me a joke about {input}\")\n",
    "prompt.save(\"prompt.yaml\")\n",
    "prompt.save(\"prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about chickens'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"prompt.yaml\")\n",
    "prompt.format(input=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cows'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = load_prompt(\"prompt.json\")\n",
    "prompt.format(input=\"cows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from google.auth import default, transport\n",
    "from langchain import PromptTemplate\n",
    "# Build\n",
    "from langchain_openai import ChatOpenAI\n",
    "from vertexai.preview import rag\n",
    "\n",
    "credentials, _ = default(scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "auth_request = transport.requests.Request()\n",
    "credentials.refresh(auth_request)\n",
    "\n",
    "\n",
    "MODEL_LOCATION = \"us-central1\"\n",
    "PROJECT_ID='sacred-alliance-433217-e3'\n",
    "MODEL_ID = \"meta/llama3-405b-instruct-maas\"  # @param {type:\"string\"} [\"meta/llama3-405b-instruct-maas\"]\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=f\"https://{MODEL_LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{MODEL_LOCATION}/endpoints/openapi/chat/completions?\",\n",
    "    api_key=credentials.token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'llm' from 'openai' (c:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'llm' from 'openai' (c:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import llm\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"topic\": \"funny\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m rapper_prompt_template: PromptTemplate \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     24\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m], template\u001b[38;5;241m=\u001b[39mrapper_template)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# creating the rapper chain\u001b[39;00m\n\u001b[0;32m     27\u001b[0m rapper_chain: LLMChain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m---> 28\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlyric\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt\u001b[38;5;241m=\u001b[39mrapper_prompt_template)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# verifier\u001b[39;00m\n\u001b[0;32m     31\u001b[0m verifier_template: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a verifier of rap songs, you are tasked\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124mto inspect the lyrics of rap songs. If they consist of violence and abusive languge\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124mflag the lyrics. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;132;01m{lyric}\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from client import llm\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "# decouple to read .env variables(OpenAI Key)\n",
    "# simple sequential chain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "# memory in sequential chain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# rapper\n",
    "rapper_template: str = \"\"\"You are an American rapper, your job is to come up with\\\n",
    "lyrics based on a given topic\n",
    "\n",
    "Here is the topic you have been asked to generate a lyrics on:\n",
    "{input}\\\n",
    "\"\"\"\n",
    "\n",
    "rapper_prompt_template: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"input\"], template=rapper_template)\n",
    "\n",
    "# creating the rapper chain\n",
    "rapper_chain: LLMChain = LLMChain(\n",
    "    llm=llm, output_key=\"lyric\", prompt=rapper_prompt_template)\n",
    "\n",
    "# verifier\n",
    "verifier_template: str = \"\"\"You are a verifier of rap songs, you are tasked\\\n",
    "to inspect the lyrics of rap songs. If they consist of violence and abusive languge\\\n",
    "flag the lyrics. \n",
    "\n",
    "Your response should be in the following format, as a Python Dictionary.\n",
    "lyric: this should be the lyric you received \n",
    "Violence_words: True or False\n",
    "\n",
    "Here is the lyrics submitted to you:\n",
    "{lyric}\\\n",
    "\"\"\"\n",
    "\n",
    "verified_prompt_template: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"lyric\"], template=verifier_template)\n",
    "\n",
    "# creating the verifier chain\n",
    "verifier_chain: LLMChain = LLMChain(\n",
    "    llm=llm, output_key=\"AI_verified\", prompt=verified_prompt_template)\n",
    "\n",
    "\n",
    "# final output chain\n",
    "final_template: str = \"\"\"You are a final quality assurance of a lyrics post.\\\n",
    "Your job will be to accept a lyric and output data in the following format\n",
    "\n",
    "Your final response should be in the following format, in a Python Dictionary format:\n",
    "lyric: this should be the lyric you received\n",
    "Date and time verified: {time_created_and_verified}\n",
    "Verified by human: {verified_by_human}\n",
    "\n",
    "Here is the lyric submitted to you:\n",
    "{AI_verified}\\\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_template: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"AI_verified\", \"time_created_and_verified\", \"verified_by_human\"], template=final_template)\n",
    "\n",
    "# creating the verifier chain\n",
    "final_chain: LLMChain = LLMChain(\n",
    "    llm=llm, output_key=\"final_output\", prompt=final_prompt_template)\n",
    "\n",
    "\n",
    "# creating the simple sequential chain\n",
    "ss_chain: SequentialChain = SequentialChain(\n",
    "    memory=SimpleMemory(memories={\n",
    "                        \"time_created_and_verified\": str(datetime.utcnow()), \"verified_by_human\": \"False\"}),\n",
    "    chains=[rapper_chain, verifier_chain, final_chain],\n",
    "    # multiple variables\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"final_output\"],\n",
    "    verbose=True)\n",
    "\n",
    "# running chain\n",
    "review = ss_chain.run(input=\"christ worship songs\")\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from client import llm\n",
    "\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | llm\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | llm\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "response=map_chain.invoke({\"topic\": \"bear\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from client import llm\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "reponse=chain.invoke({\"question\": \"how do I call Anthropic?\"})\n",
    "print(reponse)\n",
    "\n",
    "langchain_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | llm\n",
    "anthropic_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | llm\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | llm\n",
    "\n",
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropic_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ") | StrOutputParser()\n",
    "\n",
    "final_reponse=full_chain.invoke({\"question\": \"how do I use Anthropic?\"})\n",
    "print(final_reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client import llm\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "reponse=composed_chain.invoke({\"topic\": \"bears\"})\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from google.auth import default, transport\n",
    "from langchain import PromptTemplate\n",
    "# Build\n",
    "from langchain_openai import ChatOpenAI\n",
    "from vertexai.preview import rag\n",
    "\n",
    "credentials, _ = default(scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "auth_request = transport.requests.Request()\n",
    "credentials.refresh(auth_request)\n",
    "\n",
    "\n",
    "MODEL_LOCATION = \"us-central1\"\n",
    "PROJECT_ID='sacred-alliance-433217-e3'\n",
    "MODEL_ID = \"meta/llama3-405b-instruct-maas\"  # @param {type:\"string\"} [\"meta/llama3-405b-instruct-maas\"]\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=f\"https://{MODEL_LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{MODEL_LOCATION}/endpoints/openapi/chat/completions?\",\n",
    "    api_key=credentials.token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     11\u001b[0m     [\n\u001b[0;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite the following equation using algebraic symbols then solve it. Use the format\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEQUATION:\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m          (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{equation_statement}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m          ]\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m runnable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     20\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequation_statement\u001b[39m\u001b[38;5;124m\"\u001b[39m:runnablepassthrough()}\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m|\u001b[39m model\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOLUTION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m|\u001b[39mStrOutputParser()\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(runnable\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx raised to the third plus seven equals 12\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chiranjeev R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"write the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:\"),\n",
    "         (\"human\",\"{equation_statement}\"),\n",
    "         ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "runnable = (\n",
    "    {\"equation_statement\":runnablepassthrough()}\n",
    "    | prompt \n",
    "    | model.bind(stop=\"SOLUTION\")\n",
    "    |StrOutputParser()\n",
    "    )\n",
    "print(runnable.invoke('x raised to the third plus seven equals 12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict): \n",
    "    return _multiple_length_function(_dict[\"text1\"],_dict[\"text2\"])\n",
    "\n",
    "model = ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain1 = prompt|model \n",
    "\n",
    "chain = (\n",
    "    {\"a\":itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "     \"b\":{\"text1\":itemgetter(\"foo\"),\"text2\":itemgetter(\"bar\")}\n",
    "     | RunnableLambda(multiple_length_function),\n",
    "     }\n",
    "     |prompt | model\n",
    "\n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\":\"bar\" , \"bar\":\"gra\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
